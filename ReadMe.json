{
  "Cloudstack": {
    "Evaluation of cloudstack 4.8.0, 4.8.0.1, and 4.9.0 all-in-one deployment(s)": "Each evaluation is performed with a single CentOS 6 host running all services, using the KVM hypervisor."
  }, 
  "Refs.": {
    "At a minimum, follow the official installation-n-config guides; and each doc includes two must-read sections entitled:": " \"Quick Installation Guide for CentOS6\" and \"Host KVM Installation\".\n", 
    "Release 4.9": " http://docs.cloudstack.apache.org/projects/cloudstack-installation/en/4.9/hypervisor/kvm.html\n and http://docs.cloudstack.apache.org/projects/cloudstack-installation/en/4.9/qig.html\n and http://docs.cloudstack.apache.org/projects/cloudstack-installation/en/4.9\n", 
    "Release 4.8": " http://docs.cloudstack.apache.org/projects/cloudstack-installation/en/4.8/hypervisor/kvm.html\n and http://docs.cloudstack.apache.org/projects/cloudstack-installation/en/4.8/qig.html\n and http://docs.cloudstack.apache.org/projects/cloudstack-installation/en/4.8/\n\n\nNote the above online docs. (4.8 and 4.9) provide a Search option -- sadly \"uuid\" or \"UUID\" yields\nonly 1 hit -- http://docs.cloudstack.apache.org/projects/cloudstack-installation/en/4.9/hypervisor/xenserver.html?highlight=uuid\nSee the below's agent.properties discussion\n\nAlso, this ref is worth a look, although a tad dated:\nhttps://cwiki.apache.org/confluence/display/CLOUDSTACK/SSVM,+templates,+Secondary+storage+troubleshooting"
  }, 
  "A. Preliminaries": {
    "0000. Decide what LAN setup to use ... bridges with (real and virtual) NICs assigned, (multiple) IPs assigned to bridges,": "  tagged VLAN or not, routing table with default gateway ... The fewer subnets the better (like 10.101.20.0 and 10.13.80.0)\n", 
    "000. Git clone day-to-day/2016/07/{5169,CoudstackAssets} -- lots of (real and pseudo) bash and python scripts and other files.": "", 
    "00. Backup /etc/sysconfig/iptables file.": "Note the ports cloudstack needs: https://cwiki.apache.org/confluence/display/CLOUDSTACK/Ports+used+by+CloudStack\nfor Apache Tomcat (HTTPD), NFS, DNS, etc., and also VNC 5800-6100\n", 
    "01. Set SELinux to Permissive: \"setenforce 0\". Review cs_yum_rpms.sh (do not attempt to run this pseudo bash script),": "and perform the indicated sets of \"yum installs\" and \"pip installs\".\n", 
    "02. After installing all deps, make sure mysqld and NetworkManager are not running:": "a.) service mysqld stop ; chkconfig mysqld off\nb.) service NetworkManager stop ; chkconfig NetworkManager off\n    (consider yum remove NetworkManager)\n", 
    "03. First install yum/rpm cloudstack-common, then the agent and management servers ... Some deps. may not be available": "via yum. For 4.9 it was necessary to download the new mysql-connector-python-*.rpm from:\n  https://dev.mysql.com/downloads/connector/python\n", 
    "04. After yum/rpm of cloudstack-management and cloudstack-agent (and/or after running the setup-managment script, see below),": "check if they are up/running, and if so, stop them:\na.) service cloudstack-agent status ; service cloudstack-agent stop ; chkconfig NetworkManager off\nb.) service cloudstack-management status ; service cloudstack-management stop ; chkconfig cloudstack-management off\n", 
    "05. yum/rpm cloudstack-cli and use python-pip for cloud(stack) modules: pip install cloudmonkey apache-libcloud": "", 
    "06. Backup the newly created directories (by the yum/rpm) /usr/share/cloudstack-common/vms and /etc/cloudstack/{agent,management}": "a.) /etc/cloudstack/agent/agent.properties -- must be hand-edited and backed-up; agent service (re)start will overwrite it\nb.) /etc/cloudstack/management/db.properties -- can be hand-edited (or use init_db bash func -- see below)\nc.) The ISO for all System VMs results in running instances must be \"patched\" after 1st-time boot-up.\n\n  4.8: -rw-------. 1 root  root  70M Jul 14 15:02 /usr/share/cloudstack-common/vms/systemvm.iso -- be sure to backup a copy.\n       -rw-rw-rw-. 1 cloud cloud 69M Jan 30  2016 /usr/share/cloudstack-common/vms/systemvm.zip\n\n  4.9: -rw-rw-rw-. 1 cloud cloud 76M Aug  2 03:40 /usr/share/cloudstack-common/vms/systemvm.iso -- seems intact after many restarts\n", 
    "07. Start the mysqld and source cs_mysql.sh: \". ./bash/cs_mysql.sh\"": "", 
    "08. The yum/rpm management post-install indicates one should manually run: /usr/bin/cloudstack-setup-management": "", 
    "09. The above creates and inits the mysql \"cloud\" db and db-account, and overwrites /etc/cloudstack/management/db.properties": "", 
    "10. The above also inserts some cloudstack iptables rules and performs iptables-save, overwriting /etc/sysconfig/iptables": "", 
    "11. The newly created iptables file lacks all annotations/comments; these should be merged into the new file from the backup.": "", 
    "12. Our cs_mysql.sh bash script defines some convenient bash functions, run: init_db": "", 
    "13. init_db ensures there is a mysql cloud db-account, and /etc/cloudstack/management/db.properties contains the proper db-account and passwords": "", 
    "14. There is no equiv. agent setup script for /etc/cloudstack/agent/agent.properties; this must be hand edited": {
      "a.) Create 2 uuids: uuidgen && uuidgen ... samples below ...": "  88bf4f6f-b542-4d71-b733-e1e0bc28542c\n  d2197860-f163-402e-8553-cce792a9cd39\n", 
      "b.) Cut-n-paste uuids into /etc/cloudstack/agent/agent.properties (specifically):": "  guid=88bf4f6f-b542-4d71-b733-e1e0bc28542c\n  local.storage.uuid=d2197860-f163-402e-8553-cce792a9cd39\n", 
      "c.) Note the agent.properties file contains zone, pod, and cluster names each set to \"default\". If these are left as-is/unedited,": "  one MUST specify \"default\" as the name for the zone, pod, and cluster setup with the Admin GUI (see below).\n  Also be sure to double-check there are no typos in the zone, pod, and cluster names. The names that are present in\n  the agent.properties files MUST be CONGRUENT with the names entered in the Admin GUI dialogues.\n", 
      "d.) The simplest edit of the agent.properties file would be just the 2 uuids. This would induce the agent to look": "  at the host network config for NICs (but it ignores bridges) -- it will then proceed to create its own bridge(s).\n  Even when one edits the agent.properties, the agent proceeds with \"cloudbr0\" and possibly \"cloudbr1\". For our\n  single host setup a single bridge suffices, so consider editing all bridge entries to == cloudbr0.\n", 
      "e.) Another edit to consider in agent.properties is the \"use local storage\", ensuring that guest VM image files": "  are placed in the standard location /var/lib/libvirt/images.\n\n"
    }
  }, 
  "B. Other system config. /etc/... and /mnt/... (the latter is NFS) files that must be modified to properly support cloudstack": {
    "00. It is IMPORTANT to note the host KVM installation indicates the NFS /mnt/secondary MUST be remain mounted at all times.": "Also it is IMPLIED that /mnt/secondary is a MANDATORY mount point (not /net/secondary or whatever), despite the\nadmin GUI dialogue that prompts the user to enter whatever. So be sure to enter this mount point name without typos\nin the Admin GUI dialogue.\n\nIt is also IMPORTANT to note that whether the management service and hypervisor agent service are running on\nthe same host or different hosts on the LAN, the system install-n-config has fewer glitches/gotchas if one\nplaces all network elements on the same subnet (ICBD 10.101.20.0/24 should be ideal). There have been innumerable\nissues with the eval. using 3 subnets -- 10.13.80.0, 10.101.8.0, and 172.0.0.0 ... Also, it is better/cleaner\nto assign IPs to bridges rather than NIC eth*s.\n\nThe KVM hypervisor docs. indicated above provide a specific detailed example of a tagged VLAN eth0.100-300\n3 (virtual) NICs and 2 bridges (cloudbr0 and cloudbr1) network config. using ifconfig (vconfig, and brctl\ncommands provide diagnostics and non-persistent config. options).\n\nOur two evaluation hosts (c8-13 and c8-14) /etc/sysconfig/network-scripts/ifcfg* are slightly different\nsimplifications of the KVM doc. example which primarily use cloudbr0 (cloudbr0-1 seem to be hardcoded in\nsome cloudstack code).\n\nThe c8-13 network is actually not setup with the tagged VLAN 100-300, it uses cloudbr0 with multiple\nIPs configured vi the iproute2-tools (not ifconfig-net-tools; see /etc/rc.local):\n\n  ip addr add 172.16.10.2/24 brd + dev cloudbr0\n\nThe c8-14 network config. uses ifconfig scripts /etc/sysconfig/network-scripts/ifcfg-eth0.100,200,300\nas described in the KVM docs., but all are assigned to the cloudbr0 bridge.\n", 
    "01. Some host system configs. and /etc file edits that seem to improve the odds of success:": {
      "a.) Rebooting with the Secondary Storage VM up-n-running can hang NFS, due to mounts being \"busy\", and": "    starting NFS at host boot can also be problematic (especially if /etc/fstab indicates NFS mounts).\n    Consequently it is safer to disable many of the cloudstack service deps. After a clean reboot:\n\n    chkconfig --list|egrep -i 'aud|cloud|iptab|virt|dnsm|mysq|network|nscd|ntp|nfs|bind|tomc'\n    auditd                  0:off   1:off   2:on    3:on    4:on    5:on    6:off\n    cloudstack-agent          0:off 1:off   2:off   3:off   4:off   5:off   6:off\n    cloudstack-ipallocator  0:off   1:off   2:off   3:off   4:off   5:off   6:off\n    cloudstack-management     0:off 1:off   2:off   3:off   4:off   5:off   6:off\n    dnsmasq                 0:off   1:off   2:off   3:off   4:off   5:off   6:off\n    iptables                0:off   1:off   2:on    3:on    4:on    5:on    6:off\n    libvirt-guests          0:off   1:off   2:on    3:on    4:on    5:on    6:off\n    libvirtd                0:off   1:off   2:on    3:on    4:on    5:on    6:off\n    mysqld                  0:off   1:off   2:off   3:off   4:off   5:off   6:off\n    network                 0:off   1:off   2:on    3:on    4:on    5:on    6:off\n    nfs                     0:off   1:off   2:off   3:off   4:off   5:off   6:off\n    nfslock                 0:off   1:off   2:off   3:off   4:off   5:off   6:off\n    nscd                    0:off   1:off   2:off   3:off   4:off   5:off   6:off\n    ntpd                    0:off   1:off   2:on    3:on    4:on    5:on    6:off\n    ntpdate                 0:off   1:off   2:off   3:off   4:off   5:off   6:off\n    rpcbind                 0:off   1:off   2:off   3:off   4:off   5:off   6:off\n    virt-who                0:off   1:off   2:on    3:on    4:on    5:on    6:off\n\n    The above hopefully ensured a fast / simple boot-up. One must then manually\n    \"service name start\" of the items listed above that indicate \"off\".\n    Notice that libvirtd should be up immediately after reboot, but any subsequent\n    edit of /etc/libvirt* conf. files will require a restart. It's worth\n    checking the default libvirtd (KVM) boot status:\n\n    virsh list ; virsh pool-list ; virsh net-list\n\n    The 1st list should show any running VMs.\n\n    There should be a \"default\" network and but no running VMs, unless\n    somehow the cloudstack services were started on boot.\n\n    If there are pools, check if the pools have volumes:\n\n    virsh vol-list any-pool-Id-shown\n\n    Below there are some check-list items for using virsh to flush / remove\n    any lingering cloudstack remnants, if desired.\n\n", 
      "b.) Before a reboot, double check /etc/rc.local:": "    c8-13:/etc/rc.local -- ip addr add 172.16.10.2/24 brd + dev cloudbr0\n    c8-14:/etc/rc.local -- ?\n", 
      "c.) After a reboot, check the route table and network config (route -n, ifconfig, etc.).": "    We need to decide if the default boot-up should enable the public / campus VLAN 989 tag. But\n    there must be a default gateway in the route table, otherwise the the cloudstack-management\n    service will fail to startup (with very obscure error logs). Once the cloudstack system\n    has been configured with a specific default gateway in the route table, changing the\n    default gateway seems to cause problems for cloudstack-management.\n", 
      "d.) Various and sundry files under /etc that have been touched during the cloudstack eval:": "    /etc/audit* -- selinux audit rules\n    /etc/init.d/cloudstack* -- startuo scripts\n    /etc/libvirt* -- conf user, group, tcp, etc.\n    /etc/{exports,hosts,hosts.allow,idmapd.conf,my.cnf,passwd,shadow,sysctl.conf} --\n      ipv4 forwarding, NFS, KVM, non-krb5 ssh logins\n    /etc/modprobe.d/ipv6.conf -- comment all lines out (ala Sir Alex)\n    /etc/polkit* -- libvirt users\n    /etc/security/* -- non-krb5 logins\n    /etc/selinux/config -- permissive vs. enforcing\n    /etc/ssh/* -- non-krb5 login\n    /etc/sysconfig/{ptables,network*} -- eth0-1, br0-1, cloudbr0-1\n    /etc/udev/rules.d/* -- ?\n    /etc/xinetd.d/* -- TBD\n    /etc/yum.repos.d/cloudstack -- yum install\n"
    }
  }, 
  "C. Initial Configuration via Admin GUI": {
    "00. Before starting the cloudstack services be sure that:": {
      "a.) Mysqld, RPCbind, and NFS services are running. NFS exports should include:": "    exportfs -v\n    /export           <world>(rw,wdelay,nohide,no_root_squash,no_subtree_check,sec=sys,rw,no_root_squash,no_all_squash)\n    /export/primary   <world>(rw,wdelay,nohide,insecure,root_squash,no_subtree_check,sec=sys,rw,root_squash,no_all_squash)\n    /export/secondary <world>(rw,wdelay,nohide,insecure,root_squash,no_subtree_check,sec=sys,rw,root_squash,no_all_squash)\n", 
      "b.) /mnt/secondary has been seeded with the Qemu-KVM qcow2 guest VM template via:": "    /usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt -m /mnt/secondary \\\n    -u http://cloudstack.apt-get.eu/systemvm/4.6/systemvm64template-4.6.0-kvm.qcow2.bz2 -h lxc -F\n    Note the above version 4.6.0 seems to be appropriate for the 4.8 and 4.9 cloudstack releases. However,\n    the 4.9 admin GUI shows for the Virtual Router System VM: Requires Upgrade \"Yes\", while the 4.8 GUI shows \"No\".\n", 
      "c.) One has run the unlock.sh script to ensure the cloudstack services have full access to the file-system. this sets selinux": "    to permissive and chmod's certain essential items.\n", 
      "d.) The route table has been configured with the desired default gateway -- 10.a.b.1 or 10.c.d.1 or 172.16.a.1 or 172.17.b.1 ...": "", 
      "e.) The cloudbr0 bridge is up and fully configured with desired management and hypervisor host IP: brtcl show and ifconfig, etc.": "    Note the KVM installation guide indicates two cloudstack specific bridges: cloudbr0 and cloudbr1 ... however,\n    in a \"basic networking\" config, only cloudbr0 is needed (simpler and perhaps more efficient)\n"
    }, 
    "01. One may wish to edit the log4j XML config. files to increase log-levels to DEBUG or TRACE:": "  /etc/cloudstack/agent/log4j-cloud.xml\nand\n  /etc/cloudstack/management/log4j-cloud.xml\n", 
    "02. Start the management service: service cloudstack-management start && tail -f /var/log/cloudstack/management/management-server.log": "", 
    "03. Navigate one's browser to the management (for a single host setup this is also the hypervisor) host URL -- http://host:8080/client": "", 
    "04. When adding a host to a cluster, the Admin GUI prompts for an account and password with the hint \"Usually root\". All": "attempts to use a sudoer account rather than root caused problems down-stream. Consequently the only successful configs\nhave occurred when using root. Note that the setup script mentioned above creates a so-called \"password-locked\" account\n\"cloud\" with group \"cloud\" that is a sudoer account. It may be of interest to give the cloud account a password and try\nusing it for adding a host to a cluster. Also note that adding a host to a cluster requires the cloudstack-agent service\nto be started and fully initialized and communicating with both the management service and libvirtd. If one has restarted\nthe management service after editing the Global Settings (see below), with the hypervisor agent up, the agent will usually\nreconnect gracefully. If the agent is not up, the next time one attempts to use the management service Admin GUI to add\na host to a cluster, the management server may attempt to start the agent (assuming it is configured to run on the same host),\nbut that can take awhile, and it's quicker to manually start the agent once the \"heartbeats\" appear in the management\nserver log.\n", 
    "05. Check the /var/log/cloudstack/agent/agent.log -- it it does not exist or is empty, one may start the agent manually:": "service cloudstack-agent start && tail -f /var/log/cloudstack/agent/agent.log\n", 
    "06. Global Settings are IMPORTANT -- navigate to the left-side-bar of the Admin GUI and click \"Global Settings\" (directly": "under \"Infrastructure\"). A very large (but searchable) table of configuration items is presented. Many of these items\nmust be manually set after the initial management serivce startup, before starting the hypervisor agent service. In most cases,\n(re)setting a Global Settings item requires a restart (or stop-then-start) of the management-server. List of essential seetings.\n\n### a.) CIDRs -- management server network, control network, guest VM\n### b.) Host -- management server host IP ... things are better behaved when this is on the same subnet as the system VMs.\n### c.) Hypervisor.list -- just KVM\n### d.) Secstorage.allowed... -- IMPORTANT \"Comma separated list of cidrs internal to the datacenter that can host template download servers\"\n                             If not properly set, the secondary storage VM will fail.\n### d.) System.vm.use.localstorage -- true\n### e.) System.vm.default.hypervisor -- KVM\n### f.) *.rpfilter -- when all cloudstack services run on same/single host may help to set these to 0 (false)\n### g.) ???\n", 
    "07. Remember, in general any Global Settings change requires a restart of cloudstack-management.": {
      "a.) make sure the proper default gateway is setup in the route table": "", 
      "b.) make sure /var/log/cloudstack permissions are open": "", 
      "c.) make sure /root/.ssh/*rsa* permissions are open": "", 
      "d.) make sure /etc/cloudstack/{agent,management} properties and xml files are correct": "", 
      "e.) make sure /usr/share/cloudstack-common/vms/systemvm.iso is intact (4.8: ~69M or 4.9: ~76M)": "", 
      "f.) make sure /mnt/{secodary,primary} are mounted rw.": "", 
      "g.) see the unlock.sh script -- selinux and chmod and?": ""
    }
  }, 
  "D. System VM Patches and Early-Config -- once the admin GUI \"Infrastructure\" shows 2 System VMs": {
    "00. Differences between release patching": {
      "a.) 4.9: /usr/share/cloudstack-common/scripts/vm/hypervisor/kvm/patchviasocket.py and": {
        "b.) 4.8: this ISO is smaller and its /usr/local/cloud/systemvm is nearly empty:": " -rw-------. 1 root  root  70M Jul 14 15:02 /usr/share/cloudstack-common/vms/systemvm.iso\n -rw-rw-rw-. 1 cloud cloud 69M Jan 30  2016 /usr/share/cloudstack-common/vms/systemvm.zip\n\n\nVirsh console usually will not work (in 4.8) until the system VMs are more fully patched.\nFortunately ssh via -p 3922 and scp -P 3922 to the system VM's link-local eth0 (169.254.x.y) work.\n\nscp /usr/share/cloudstack-common/vms/systemvm.zip into each system VM (/var/tmp) and then\nssh login and unzip into either /usr/local/cloud/systemvm or /opt/cloud/systemvm and sym-link\none to the other. Then run: service cloud-early-config (re)start and service (re)start and\ncheck /var/log/cloud.log for any errors, etc.\n\nOnce /usr/local/cloud/systemvm/* files are installed, (re)run / (re)start the cloud services:"
      }, 
      "a.) service cloud-early-config restart -- this may cause a reboot of the VM": "", 
      "b.) service cloud restart -- check /var/log/cloud.log for errors, exceptios, etc:": "  egrep -i 'abor|canno|erro|excep|fail|fata|unable' /var/log/cloud.log\n"
    }, 
    "01. Once the system VMs have been patched, try rebooting each via the admin GUI. The GUI needs to": "be refreshed manually to observe changes in state/status of the system VMs.", 
    "02. Once the System VMs are fully booted, try to virsh console into each. Virsh list should show": "VM names s-*-VM for the secondary-storage VM, v-*-VM for the console-proxy VM, r-*-VM for\nany virtual-router VM, and i-*-VM for any guest VMs. The virtual router VM (r-*-VM) does not\nappear to be created and booted by the system until one attempts to launch a guest VM.", 
    "03. Note the system VMs are Debian 7 (wheezy) and are running sshd and iptables. It may be necessary": {
      "a.) service ssh restart": "", 
      "b.) service iptables-persistent restart": ""
    }, 
    "04. Each fully patched system VM should have a verification test script one can run:": "/usr/local/cloud/systemvm/ssvm-check.sh\n\nIn order for the ssvm-check script to work fully, however, the VM needs access to the Internet.\nThe network config. across the host and the system VMs needs to be setup to allow access to the\nInternet -- the UF campus bridge br1 and VLAN tag eth1.989 are up , and the VMs can route across\ncloudbr0 (thru br0?) thru br1 to the Internet. During the \"early-config\" and subsequent post-install\nconfig., and then using guest VMs, it is possible to achieve considerable functionality without\nInternet access. Nevertheless, it is highly desirable to provide some level of Internet access from\nour private cloud.", 
    "05. In the event our manual patch effort or some other activity has damaged a system VM beyond repair,": {
      "a.) service cloudstack-agent stop ; service cloudstack-management stop": "", 
      "b.) virsh list -- note VM names, if any are shown and try:": "  virsh shutdown each-VM-name -- or destroy or undefine\n", 
      "c.) virsh pool-list -- and note any/all the pool Ids": "", 
      "d.) virsh vol-list each-pool-name -- note all the vol-names": "", 
      "e.) for each vol of each pool: virsh vol-delete vol-name pool-name": "", 
      "f.) for each pool-name: virsh pool-delete pool-name": "Optionally copy/backup the current log files and then \"truncate -s 0\" all logs and restart the services:\n\nservice cloudstack-management start\n\n\nMonitor the management-server.log for awhile to see the \"heartbeats\":\n\ngrep -i heartbeat /var/log/cloudstack/management/management-server.log|tail -2\n2016-08-18 17:53:47,377 INFO  [o.a.c.f.j.i.AsyncJobManagerImpl] (AsyncJobMgr-Heartbeat-1:ctx-49311184) (logid:a4ca6d21) Begin cleanup expired async-jobs\n2016-08-18 17:53:47,381 INFO  [o.a.c.f.j.i.AsyncJobManagerImpl] (AsyncJobMgr-Heartbeat-1:ctx-49311184) (logid:a4ca6d21) End cleanup expired async-jobs\n\n\nAfter a few minutes, if the above grep fails to find any heartbeats, we have a problem. But if we see heatbeats, then\nrefresh (re-login) to eh Admin GUI and proceed with the agent restart:", 
      "a.) service cloudstack-agent start -- and after many many minutes Admin GUI Infrastructure will show 2 (new) System VMs": "", 
      "b.) Click thru to the System VM page and monitor their status by refreshing the page. Eventually thet status should change": "  to green \"Running\". Note their names and IPs.\n", 
      "c.) Try to virsh console into each VM-name, or ssh -P 3922 into each VM's link-local IP.": "", 
      "d.) If one can login as root (password), check the contents of each VM's /usr/local/cloud/systemvm": ""
    }
  }, 
  "E. Registration of Templates (qcow2 images) and ISOs.": {
    "00. Start a http web-server on the host that provides file downloads via URLs.": "", 
    "01. Navigate to the Admin UI left-side-bar and to the Templates page.": "", 
    "02. The Template pull-down allows one to either upload a template or specify a URL, but the upload": "will not work from our iMAC desktops due to the CIDR network config. A URL of the management host\nwill work if we have an http web-server running on the host (and proper Global Settings).\n", 
    "03. The ISO registration only supports URLs, again we need a http web-server up on the host.": "", 
    "04. Make sure to select/enable \"Featured\" and \"Shared\" in the registration dialogue.": "The GUI will blink for a short while then a pop-up appears indicating success (or not).\nBut the success indicated is premature. It actually takes considerably longer for the\nsystem to make the new offering available. Click thru \"Add Instance\" and \"Template\" or \"ISO\"\nand \"Featured\" a few times and eventually the new item will be shown in the list. Sometimes\na new item can appear in a tab then be removed later by the system for mysterious reasons\n(perhaps SELinux)?\n"
  }, 
  "F. Launch and Use a Guest VM": {
    "00. Navigate to the Admin or User GUI left-side-bar and click \"Instances\", then click \"Add Instance\".": "", 
    "01. A page is presented that allows one to navigate / choose \"Featured\" or \"Community\" or \"Shared\"": " Templates or ISOs from their respective lists.\n", 
    "02. Once the new (featured) ISO or Template is selected, proceed with the subsequent steps and launch.": {
      "a.) Setup -- zone and ISO or Template buttons": "", 
      "b.) Select -- specific ISO or Template (qcow2 image for KVM hypervisor) from \"Featured\" or other tab.": "", 
      "c.) Compute Offering -- small or medium (RAM, CPUs, etc.)": "", 
      "d.) Disk Offering == small, medium, large, custom, etc.": "", 
      "e.) Affinity -- no affinity groups are defined when there is only 1 hypervisor host on 1 cluster (our only possible affinity).": "", 
      "f.) Network -- in \"basic networking\" config there is only 1 network choice (\"default\")": "", 
      "g.) SSH KeyPair -- none; each guest VM should ultimately establish unique pairs per user.": "", 
      "h.) Review and Launch -- provide an optional name for the instance.": ""
    }, 
    "03. Navigate back to the GUI \"Instances\" page and refresh it a few time to observer the table of": " instances update and eventually show its dynamically allocated IP address and its \"State\"\n become green \"Running\".\n", 
    "04. Click on the instance name shown in the table to navigate to a page that present 4 tabs.": "", 
    "05. The named instance page tab \"Details\" provides a scrollable list that shows VM info.": " The \"NICs\" tab shows the guest VM's network config: IP, netmask, default gateway, etc.\n There is also a \"Security Groups\" and \"Statistics\" tab, etc.\n", 
    "06. Once the guest VM is fully booted, navigate to the \"Instances\" page in the User or Admin GUI": " and click into the instance \"Details\" tab. Note the row of small icons shows (the rightmost)\n one that looks like \">_\". Hover the mouse over the icon and a pop-up label should appear:\n \"View console\". To access the guest VM, click on the icon and a new browser window should appear\n and indicate it is attempting to connect to the guest VM via the Console Proxy system VM IP.\n If the browser app. is running on the hypervisor host. or some other host that has unrestricted\n network access to the VMs, the window should display the VM's OS console. If the guest VM derives\n from a \"live\" ISO, the console will likely be a Desktop GUI. If the guest VM derives from an install\n ISO, the console should display a typical install (text or GUI) prompt.\n\n If the only network route to the guest VM is the hypervisor host, viewing the VM console via the\n cloudstack wep-app User or Admin browser interface requires running the browser directly on the\n hypervisor bare-metal OS, via X11 forwarding. Consequently some X11 (client) deps. must be installed,\n and the sshd config. must support the forwarding. An X11 client package install (yum/rpm) may result\n in NetworManager being installed and started. The NetworkMager service may interfere with the network\n config that has been manually created. It seems best to disable NetworkManager, and remove it altogether.\n\n Note there is known bug in the cloudstack console poxy, as described here:\n\n https://issues.apache.org/jira/browse/CLOUDSTACK-9164\n\n The above describes a manual patch for the console's \"ajaxviewer.js\" that should be found in the VM's\n /usr/local/cloud/systemvm/js sub-directory.\n\n Copy (scp) the patchfile (prevent_quick_search_key.patch) to the hypervisor host from the git cloned\n directory that also contains this cs_checklist.txt. Then on the hypervisor copy (scp -P 3922) the\n patchfile to the Console Proxy System VM: /usr/local/cloud/systemvm/js. Then ssh or virsh console to\n the VM and pushd there to perform the patch:\n\n cp -p ajaxviewer.js ajaxviewer.js.orig\n patch < prevent_quick_search_key.patch\n diff ajaxviewer.js ajaxviewer.js.orig\n\n Presumably closing and re-opening the Console Proxy browser window will load the new version, but if not,\n clear the browser internal cache and retry.\n", 
    "07. Once launched, the VM life-cycle is somewhat independent of the cloudstack services. Stopping the": "hypervisor agent and/or the managment server does not stop the VMs. VMs can be be shutdown or rebooted\nor destroyed and expunged from the GUI, and as mentioned above if necessary via virsh commands. Remember\nthat libvirtd dynamically inserts new VM routing rules in iptables, but doe not persist the rules to /etc.\nIf one restarts iptables with the VMs still running, but without first saving/persisting their iptable\nentries, connectivity may be lost. Rebooting the VMs should induce libvirtd to (re)create the iptable rules\nfor the (NAT and masquerade, etc). But it is recommended to manually persisit the iptables rules after\neach new VM is launched, ensuring iptables restarts yield consistent outcomes."
  }
}
